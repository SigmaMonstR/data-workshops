---
title: 'Session 2: Model Validation and Prediction'
output: html_notebook
---

### Introduction

Today's session is focused on the prediction process. It's really easy to make a prediction. It's really hard to get it right. 

![](img/xkcd-loss.png)

By the end of today's session, we will have covered:

- What steps you can take to make sure you have a good prediction through *model validation*
- How to balance interpretability with accuracy
- What are the different prediction methods that you can use

### Setting up the data

We'll borrow some basic data from the Quarterly Services Survey and see if we can predict services sector growth. Before we start, let's load in the data, which is a long-form data set of quarterly growths:

- *growth*: Growth by industry from the Census Quarterly Services Survey
- *ces*: by employment level by industry from the BLS Current Employment Survey 
- *google*: search query volume from Google Trends
- *cpi*: by consumer price index

The version of the data we're using is all publicly available, scraped from web sources. In total, the data contains $n = 55$, which covers the great recession, with $k = 793$ variables for 20 industries. There are three other data frames available in this data set, but we just need to worry about *qss_main*.

```{r, warning=FALSE, message = FALSE}
#LOAD DATA
  load("example.Rda")
  dim(qss_main)
```

For now let's just focus on one industry: *NAICS 511 - Publishing Industries (except internet)*. We can subset the industry by using row index notation.
```{r, warning=FALSE, message = FALSE}
#SET UP DATA
  short <- qss_main[qss_main$naics == "511",]
```

For some context, we'll create a vector that flags when the recession took place.

```{r, warning=FALSE, message = FALSE}

#Create vector
  recession <- rep(0, nrow(short))
  recession[short$period %in% c("2007-Q4", "2008-Q1", "2008-Q2",
                                "2008-Q3", "2008-Q4",
                                "2009-Q1", "2009-Q2")] <- 1
  
#Define the dimensions of a box to highlight recession in a graph
  xleft <- which(recession == 1)[1]
  xright <- which(recession == 1)
  xright <- max(xright)
  ytop = max(short$growth) + 1
  ybottom = min(short$growth) - 11
  
```

Then, plot the data to show that they are seasonal and dip during the recession.

```{r}

  plot(short$growth, type = "l", ylab = "growth", col = "orange")
  points(short$growth, col = "orange", pch = 16)
  rect(xleft, ybottom, xright, ytop, 
       col = rgb(1,1,0,0.3),
       border = FALSE)
  
```

### Refresher 

Prediction relies on the idea of *supervised learning*. Basically:

$$y = f(x)$$

indicates that we have some set of input variables $x$ that are mapped using some function $f$ to mimic a target variable $y$. It is called supervised as an algorithm has a specific goal or target to meet, doing so by learning the pattern between $y$ and $x$. The algorithm can take any data in the same structure and format as $x$, then produce a prediction $\hat{y}$. Prediction and forecasting are thus the same thing -- an algorithm learns the patterns then applies it.


### Types of models

The function $f$ can take on a number of forms, but today we'll cover two principal types: 

- __Linear regression__ assumes that the the target is linearly associated with the inputs subject to a constraint. Common examples include *ordinary least squares* (OLS), *ARIMA*, and *Least Absolute Shrinkage and Selection Operator* (LASSO). 
- __Tree-Based Learning__ partition samples into smaller and smaller more homogeneous subsamples -- basically a combination of characteristics define a cell of observations that take on an average value of the outcome. Common examples include *regression trees* and *random forests*. 

Each methods has its own assumptions as well as strengths and weaknesses.

```{r, echo = FALSE, warning=FALSE, message = FALSE, fig.height = 6}

#Set up random Data
  y <- sin((1:100)/4)/2 + (1:100)*0.05 + runif(100)
  df <- data.frame(y, x = 1:100, m = 1:100 %% 28)
  
#Plot
  par(mfrow = c(2,2))
  
  #Base example
  plot(df$y, col = "orange", pch = 16, ylab = "y", xlab = "time", main = "(1) Raw Data")
  lines(df$y, col = "orange")
  
  #OLS
  plot(df$y, col = "orange", pch = 16, ylab = "y", xlab = "time", main = "(2) OLS")
  lines(predict(lm(y ~ x, data = df)), col = "blue")
  
  #Rpart
  library(rpart)
  plot(df$y, col = "orange", pch = 16, ylab = "y", xlab = "time", main = "(3) Regression Tree")
  lines(predict(rpart(y ~ x, data = df, cp = 0)), col = "blue")
  
  #Random Forest
  library(randomForest)
  plot(df$y, col = "orange", pch = 16, ylab = "y", xlab = "time", main = "(4) Random Forest")
  lines(predict(randomForest(y ~ x, data = df)), col = "blue")
  
```

### A quick word about time series

In time series, certain behaviors are expected of the data. In particular, we want to see data that are *stationary* -- or data in which the mean, variance and autocorrelation are constant over time. In other words, if we can ensure that the  statistical properties of the data are "tame" and predictable, any paradigm we identify through modeling is generalizable. The series should not be floating off to oblivion uncontrollably,  but rather should stay relatively stable wthin certain bounds.


```{r, warning = FALSE, message = FALSE, echo = FALSE}
#Create synthetic series
  y <- (5*sin(1:100) + runif(100)*5 + (1:100)^2)

#Plot
  par(mfrow = c(1,3))

#Plot data
  plot(y, type = "l", ylab = "y", col = "blue", main = "(1) Non-Stationary")
  plot(diff(y), type = "l", ylab = "y", col = "blue", main = "(2) Differenced - Stationary")
  plot(diff(diff(y)), type = "l", ylab = "y", col = "blue", main = "(3) 2nd Difference - Stationary")

```

### Testing for stationarity

Check the *autocorrelation function* and *partial autocorrelation function*. The lags should drop off quickly if the series is stationary. Easy way to turn a non-stationary series into stationary is to take the difference using `diff()`. 

__TRY THIS__
```{r, }
#Create synthetic series
  y <- (5*sin(1:100) + runif(100)*5 + (1:100)^2)

#Plot
  par(mfrow = c(2,3), oma = c(1,1,1,1))

#Autocorrelation Function
  acf(y);
  acf(diff(y))
  acf(diff(diff(y)))


#Partial Autocorrelation Function
  pacf(y)
  pacf(diff(y))
  pacf(diff(diff(y)))
```

### Testing for stationarity: ADF 

Then also the Augmented Dickey Fuller test that tests for unit roots where the null hypothesis is that there is a presence of a unit root.

```{r}
  library(tseries)
  adf.test(y)
  adf.test(diff(y))
  adf.test(diff(diff(y)))
```

### Time Series Tips

- Testing for autocorrelation matters when dealing with models in which we intended to regress the series against itself (e.g. ARIMA)
- Spurious correlation in levels often times will overstate strength of relationships. Differencing will provide a clearer gauge of modeled relationships by removing the momentum that is embedded in variables 

### Model formation

- Contemporaneous: $y_t = f(x_t)$
- Time series/Momentum: $y_t = f(y_{t-1}, x_{t-1})$

### Linear Regression - Contemporaneous

OLS regression is the quantitative workhorse in most fields. The technique is a statistical method that estimates unknown parameters by minimizing the sum of squared differences between the observed values and predicted values of the target variable. To better understand arguably the most commonly used supervised learning method, we can start by defining a regression formula:

$$y_i = \beta_0 x_{i,0} + \beta_{1} x_{i,1} + ... + \beta_{k} \beta_{i,k} + \epsilon_{i}$$

where:

- $y_i$ is the target variable or "observed response"
- $\beta_{k}$ are coefficients associated with each $x_k$. Each coefficient can be obtained by solving $\hat{\beta} = (X'X)^{-1}X'Y$. Note that $w$ may be substituted with $\beta$ in some cases. 
- $x_{i,k}$ are input or independent variables 
- subscript $i$ indicates the index of individual observations in the data set
- $k$ is an index of position of a variable in a matrix of $x$
- $\epsilon_{i}$ is an error term that is assumed to have a normal distribution of $\mu = 0$ and constant variance $\sigma^2$

Note that $x_{i,0} = 1$, thus $w_0$ is often times represented on its own. For parsimony, this formula can be rewritten in matrix notation as follows:

$$y = X\beta + \epsilon$$
such that $y$ is a vector of dimensions $n \times 1$, $X$ is a matrix with dimensions $n \times k$ regressors, and $W$ is a vector of coefficients of length $k$, containing an intercept and a coefficient corresponding to each of $k$ features. $\beta$ is oftem times represented with $\beta$.

$$TSS = argmin (\sum^n_{i=1}(y_i - \sum^k_{j=1} x_{ij}\beta_j)^2$$


Given this formula, the objective is to minimize the Total Sum of Squares (also known as Sum of Squared Errors):

$$TSS = \sum_{i=0}^{n}{(y_i - \hat{y}_i)^2} $$
Which can also be written by substituting the prediction formula that yields $\hat{y}$:

$$TSS = \sum^n_{i=1}(y_i - \sum^k_{j=1} x_{ij}w_j)^2$$

where $k$ is the total number of variables and $j \in k$. More commonly, TSS is better contextualized as the Mean Squared Error $$MSE = \frac{1}{n}\sum_{i}^{n}{(y_i - \hat{y_i})^2}$$. The SSE and MSE are measures of uncertainty relative to the observed response. Minimization of least squares can be achieved through a a method known as *gradient descent*.


*Running a linear model* is simple using the `lm()` function regressing $growth = f(ces1, google1)$. After doig so, we can extract the coefficients to interpret the relationships. 


```{r}
#TRY RUNNING THIS and SUBSTITUTE VARIABLES IN
#Estimate model
  fit <- lm(growth ~ ces1 + google1, data = short)

#Interpret
  summary(fit)
```


And prediction is simple enough, requiring the `fit` object and dataset to which the linear model is applied.  


```{r}
#Predict 
  yhat <- predict(fit, short)
  
#Check
  plot(short$growth, type = "l")
  lines(yhat, col = "red")
```

To calculate accuracy, we can write out the formula for RMSE:

```{r}
  sqrt(mean((yhat - short$growth)^2))

```

The challenge with any plain linear regression is its inability to accommodate scenarios where there are more $k > n$. Also note that there many basic assumptions that determine whether an estimated model is valid -- thus OLS is easy to estimate, but hard to meet the assumptions. For example, linear regression does not yield an invertible solution when there are more variables than records. Secondly, one might not be able to manually test all specifications. 

##LASSO
 
One way to deal with cases where $k > n$ is using *regularized regression*. As we know with linear regression, the goal is to estimate $\beta$ by minimizing the Total Sum of Squares (TSS):

$$\hat{w} = (X'X)^{-1}X'Y$$

where each $X$ and $Y$ is standardized (mean 0 with unit variance). If we assume that there is a limited amount of TSS, then we can force linear regression to make trade offs between coefficients by introducing a _bias_ term -- a constraint $\lambda I_k$ can be added to the diagonals of $X'X$:

$$\hat{w} = (X'X + \lambda I_k)^{-1}X'Y$$.

$\lambda$ is a tuning parameter -- a value that cannot be solved for and must be uncovered through cross-validation. Depending on how much bias is added into the model determines how much "wiggle room" coefficients have to move. Expanding upon the typical TSS, $\lambda$ is tacked onto the end of the formula as a scalar of the sum of squared coefficients (an _L2-norm_):  


$$TSS_{penalized} = \sum^n_{i=1}(y_i - \sum^k_{j=1} x_{ij}w_j)^2 + \lambda \sum^k_{j=1}|w_j|^2$$

The second part of the penalized TSS can be viewed as a constraint. Given the standard TSS $\sum^n_{i=1}(y_i - \sum^k_{j=1} x_{ij}w_j)^2$, we place a constraint such that $$\sum^k_{j=1}|w_j|^2 < c $$

where $c > 0$. Taken together, the interpretation of $\lambda$ is that it regulates the size of coefficients $w$: As the size $\lambda$ is increased, the magnitude of coefficients are reduced.  The value of $\lambda$ is determined through a _grid search_ in which values of $\lambda$ are tested at equal intervals to identify the value that minimizes some error measure such as MSE. This specific formulation of a regularization is known as a _ridge regression_ and helps to bound coefficients to a "reasonable" range, but does not zero out coefficient values. 

In order to surface the most "influential" features, we may rely on a close cousin of ridge regression known as _Least Absolute Shrinkage and Selection Operator_ or _LASSO_, which relies on an _L1-norm_:
$$\sum^k_{j=1}|w_j| < c$$. By swapping a L2-quadratic constraint with a L1, least squares will behave differently.


*Run a LASSO*. 

All data needs to be in vector or matrix form.

```{r}

 #Overfit for GLMNET and plot
  library(glmnet)

 #Set up data in matrix form
  x <- as.matrix(short[,4:ncol(short)])
  y <- short$growth
```

`cv.glmnet` will find the value of lambda for you by running the model 100 different times. The value of alpha controls whether LASSO is used (1) or Ridge is used (0). Remember that LASSO will conduct variable selection and 'zero out' noisy variables, but Ridge is robust to collinearity.

```{r}
 #Model
  fit <- cv.glmnet(x, y, alpha = 1)
  
```

We can find the optimal lambda by plotting the model object
```{r}
 plot(fit)
```

And also take a peek at the coefficients of the best model
```{r, echo = FALSE}
  coef.cv.glmnet(fit, s = "lambda.min" )
```

And predict values of $y$.
```{r}
  yhat_1 <- predict(fit, x, s = "lambda.min")
  
  plot(y, pch = 16, col = "darkgrey")
  lines(y, col = "darkgrey", lty = "dashed")
  lines(yhat_1, col = "red")
```

##ARIMA 

Lastly, ARIMA does not need much introduction, other than it is a linear model that is built on lags of past values. Thus, ARIMA carries momentum and seasonal patterns forward.

```{r}
 #Create time series object
    qss <- ts(short[,3:ncol(short)], start = c(2004,3), frequency = 4)
    
  #ARIMA
  # Linearity
    library(forecast)
    fit <- auto.arima(y = qss[,1], 
                      seasonal = TRUE)
    
    #Apply regression model to out-of-sample set
    out <- forecast(fit)
    plot(out, main = "ARIMA forecast")

```

## Tree-Based Methods

Tree-based methods are not like regression. They don't have natural probabilistic properties and are considerd to be *non-parametric*

### Regression Trees

Trees are built following this algorithmic logic:

```
  1. Let Sample = S, Target = Y, Input Features = X
  2. Screen records for cases that meet termination criteria.
        If each base case that is met, partition sample to isolate homogeneous cases.
  3. For each X:
        Calculate the attribute test comparing all X's and Y
  4. Compare and identify Xi that yields the greatest separability
  5. Split S using input feature that maximizes separability
  6. Iterate process on steps 3 through 5 until termination criteria is met
```

The result is a set of conditions and inequalities that describe small subsamples of S where the target $Y$ is more homogeneous. If one follows the algorithmic logic all the way through, you will wonder how does the tree know how to stop? A tree that is fully grown will have terminal nodes (leafs) with one observation -- it might be overly complex. The tree can be *pruned* to ensure that the splits are meaningful.

```{r, message = FALSE, warning = FALSE}
  
#Rpart
  library(rpart)
  library(rpart.plot)

#Plot!
  par(mfrow = c(1,2))
  fit <- rpart(y ~ x, data = df, cp = 0)
  rpart.plot(fit, main = "Max complexity")
  
  fit <- rpart(y ~ x, data = df, cp = 0.1)
  rpart.plot(fit, main = "Pruned")
```

To prune the tree, we can first estimate the model at full complexity $cp = 0$, then take a look at the complexity table. We would like to keep the $cp$ value that corresponds with the lowest `xerror` (cross validation error).

```{r, message = FALSE, warning = FALSE}
  
#Rpart
  set.seed(123)
  fit <- rpart(y ~ x, data = df, cp = 0)
  printcp(fit)
  
#Choose lowest cp
  min.cp <- fit$cptable[which(fit$cptable[,4] == min(fit$cptable[,4]))[1],1]
  
#Re-estimate model
  fit <- rpart(y ~ x, data = df, cp = min.cp)
  yhat_dt <- predict(fit, x)
```

While trees conduct variable selection, there are issues with decision trees: 

- They tend to overfit and produce noisy estimates
- The algorithm will crash if there are missing right-hand side values
- Predictions are jagged

### Random Forest

Random Forests are much of the same, except using hundreds of trees with a twist. The technique, as crystallized in Breiman (2001), is an extension of decision trees using a modified form of bootstrapping and ensemble methods to mitigate overfitting and bias issues. Not only are individual records bootstrapped, but input features are bootstrapped such that if $K$ variables are in the training set, then $k$ variables are randomly selected to be considered in a model such that $k < K$. Each bootstrap sample is exhaustively grown using decision tree learning and is left as an unpruned tree. The resulting predictions of hundreds of trees are ensembled. The logic is described below.

__Pseudo-code__
```
Let S = training sample, K = number of input features
  1. Randomly sample S cases with replacement from the original data.
  2. Given K features, select k features at random where k < K.
  3. With a sample of s and k features, grow the tree to its fullest complexity.
  4. Predict the outcome for all records.
  5. Out-Of-Bag (OOB). Set aside the predictions for records not in the s cases.
Repeat steps 1 through 5 for a large number of times saving the result after each tree.
Vote and average the results of the tree to obtain predictions. 
Calculate OOB error using the stored OOB predictions. 
```

The *Out-Of-Bag* (OOB) sample is a natural artifact of bootstrapping: approximately one-third of observations are naturally left un-selected, which can be used as the basis of calculating each tree's error and the overall model error. Think of it as a convenient built in test sample.


```{r, message = FALSE, warning = FALSE}
  #Random Forest
  library(randomForest)
  fit.tune <- tuneRF(x = short[,4:ncol(short)], y =  short[,3], ntreeTry = 500, 
                   mtryStart = 1, stepFactor = 2, 
                   improve = 0.001, plot = TRUE)
  opt.try <- fit.tune[fit.tune[,2] == min(fit.tune[,2]), 1]
  fit <- randomForest(growth ~ . , 
               data = short,
               mtry = opt.try)
  
```

# Model Validation

How do we know that what we're dealing with is accurate? In a classroom, we would never trust test scores if students already had seen the exam answer key, would we? Same applies to prediction. Model validation is a way to check our answers. 


### Overfitting example

Hammering in the point
```{r}
  #Example of overconfidence
  #Cut out data
  y <- short$growth
  x <- as.matrix(short[,4:ncol(short)])
  
  #Plot for example
  plot(y, pch = 16, col = "darkgrey")
  lines(y, col = "darkgrey")
  
  #Overfit for GLMNET and plot
  library(glmnet)
  fit <- cv.glmnet(x, y, alpha = 1)
  yhat_1 <- predict(fit, x, s = "lambda.min")
  
  #Looks too good to be true, right?
  plot(y, pch = 16, col = "darkgrey")
  lines(y, col = "darkgrey", lty = "dashed")
  lines(yhat_1, col = "red")
```

##Train-Test design

One way to get around it is to split the sample into two or more pieces, train the algorithm on one part and use the other part as an out of sample test. Notice that the test error is much larger than the train? This indicates that the patterns that an algorithm can learn over time may change.
```{r}
  #Example 1: Train/Test
  test.start <- 15
  fit <- cv.glmnet(x[1:(test.start-1),], y[1:(test.start-1)], alpha = 1)
  
  #Predict
  yhat_2 <- predict(fit, x, s = "lambda.min")
  
  #Calculate RMSE
  sqrt(mean((yhat_2[1:(test.start-1)] - y[1:(test.start-1)])^2))
  sqrt(mean((yhat_2[(test.start):length(y)] - y[(test.start):length(y)])^2))
```

Compare overfit with testing
```{r}
  #Plot
  plot(short$growth, type = "l", lty = "dashed")
  lines(yhat_1, col = "red")
  lines(c(rep(NA, (test.start-1)),yhat_2[test.start:length(y)]), col = "blue")
```

Compare in-sample vs out-of-sample
```{r}

  plot(y, pch = 16, col = "darkgrey")
  lines(y, col = "darkgrey", lty = "dashed")
  lines(yhat_1, col = "red")
  lines(c(rep(NA, (test.start-1)),yhat_2[test.start:length(y)]), col = "blue")
```


##One Step Ahead

Problem with train/test: you will lose a lot of observations for testing. Also, for time series where the pattern changes constantly and for small samples, by blocking out a large chunk for testing is likely to take a lot of statistical power from measuring accuracy.

One step ahead validation means each observation is treated as a test, meaning that if there are $n$ test samples that there are $n$ models that need to be run.


```{r}

#Create a placeholder vector
  yhat_3 <- yhat_2
  
#Loop through each test observation, run a model
  for(i in test.start:length(y)){
    fit <- cv.glmnet(x[1:(test.start-1),], y[1:(test.start-1)], alpha = 1)
    temp <- predict(fit,x, s = "lambda.min")
    yhat_3[i] <- temp[i]
  }
  
#Compare train vs test
  sqrt(mean((yhat_3[1:(test.start-1)] - y[1:(test.start-1)])^2))
  sqrt(mean((yhat_3[(test.start):length(y)] - y[(test.start):length(y)])^2))
  
#Plot to see how it went
  plot(y, pch = 16, col = "darkgrey")
  lines(y, col = "darkgrey", lty = "dashed")
  # lines(yhat_1, col = "red")
  lines(c(rep(NA, (test.start-1)),yhat_2[test.start:length(y)]), col = "lightblue")
  lines(c(rep(NA, (test.start-1)),yhat_3[test.start:length(y)]), col = "green")

```